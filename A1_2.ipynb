{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CjK2Q5uvwoMD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XjwPtRL3xrbG"
      },
      "outputs": [],
      "source": [
        "class network:\n",
        "\n",
        "  # Arguments to constructor\n",
        "  #1 No. of nuerones in input layer\n",
        "  #2 No. of nuerones in output layer\n",
        "  #3 Hidden layer nuerons vector e.g, [128, 64, 32]\n",
        "  #4 Activation function : sigmoid, tanh, relu\n",
        "  #5 Optimizer : sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "  #6 epochs\n",
        "  #7 eta : Learning rate\n",
        "  #8 Batch Size\n",
        "  #9 Weight Decay : default value 0\n",
        "  #10 gamma : for momentum, nesterov\n",
        "  #11 beta1 : for adam, nadam\n",
        "  #12 beta2 : for adam, nadam and rmsprop\n",
        "  #13 eps : Epsilon for Learning rate decay\n",
        "  def __init__(self, il, ol, hls, activation, optimizer, epochs, eta, batch_size, w_decay = 0, gamma = 0.9, beta1=0.9, beta2 = 0.999, eps = 1e-8):\n",
        "\n",
        "    self.il_nodes = il\n",
        "    self.ol_nodes = ol\n",
        "    \n",
        "    self.h_layers = len(hls)\n",
        "    self.w = []\n",
        "    self.b = []\n",
        "\n",
        "    # for momentum and nesterov\n",
        "    self.updateW = []\n",
        "    self.updateB = []\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # for rmsprop\n",
        "    self.vW = []\n",
        "    self.vB = []\n",
        "    self.beta2 = beta2\n",
        "    self.eps = eps\n",
        "\n",
        "    # for adam\n",
        "    self.mW = []\n",
        "    self.mB = []\n",
        "    self.beta1 = beta1\n",
        "\n",
        "    self.act = activation\n",
        "    self.opt = optimizer\n",
        "    self.epochs = epochs\n",
        "    self.eta = eta\n",
        "    self.w_decay = w_decay\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    prev = il\n",
        "    np.random.seed(10)\n",
        "    for i in range(len(hls)):\n",
        "\n",
        "      self.w.append( np.random.rand(hls[i], prev) - 0.5 )\n",
        "      self.b.append( np.random.rand(hls[i], 1) - 0.5 )\n",
        "      self.updateW.append(self.w[i]*0.0)\n",
        "      self.updateB.append(self.b[i]*0.0)\n",
        "      self.vW.append(self.w[i]*0.0)\n",
        "      self.vB.append(self.b[i]*0.0)\n",
        "      self.mW.append(self.w[i]*0.0)\n",
        "      self.mB.append(self.b[i]*0.0)\n",
        "      prev = hls[i]\n",
        "\n",
        "    self.w.append( np.random.rand(ol, prev) - 0.5 )\n",
        "    self.b.append( np.random.rand(ol, 1) - 0.5 )\n",
        "    self.updateW.append(self.w[len(hls)]*0.0)\n",
        "    self.updateB.append(self.b[len(hls)]*0.0)\n",
        "    self.vW.append(self.w[len(hls)]*0.0)\n",
        "    self.vB.append(self.b[len(hls)]*0.0)\n",
        "    self.mW.append(self.w[len(hls)]*0.0)\n",
        "    self.mB.append(self.b[len(hls)]*0.0)\n",
        "\n",
        "  def network_shape(self):\n",
        "\n",
        "    print('No. of nodes at each layer')\n",
        "    print(f'{self.il_nodes}, ',end='')\n",
        "    for i in range(self.h_layers):\n",
        "      print(f'{self.w[i].shape[0]}, ',end='')\n",
        "    print(f'{self.ol_nodes}')\n",
        "\n",
        "  def weights_shape(self):\n",
        "\n",
        "    print('Weight matrices shapes are')\n",
        "    for i in range(self.h_layers):\n",
        "      print(f'{self.w[i].shape}')\n",
        "    print(f'{self.w[len(self.w)-1].shape}')\n",
        "\n",
        "  def biases_shape(self):\n",
        "\n",
        "    print('Bias vectors shapes are')\n",
        "    for i in range(self.h_layers):\n",
        "      print(f'{self.b[i].shape}')\n",
        "    print(f'{self.b[len(self.b)-1].shape}')\n",
        "\n",
        "  def vSigmoid(self, v):\n",
        "    s = 1 / (1 + np.exp(-v))\n",
        "    return s\n",
        "\n",
        "  def vSigmoid_deriv(self, v):\n",
        "    d = np.exp(-v) / (1 + np.exp(-v))**2\n",
        "    return d\n",
        "\n",
        "  def vTanh(self, v):\n",
        "    t = (np.exp(v) - np.exp(-v)) / (np.exp(v) + np.exp(-v))\n",
        "    return t\n",
        "  \n",
        "  def vTanh_deriv(self, v):\n",
        "    e = (np.exp(v) - np.exp(-v)) / (np.exp(v) + np.exp(-v))\n",
        "    d = 1.0 - e**2\n",
        "    return d\n",
        "\n",
        "  def vRelu(self, v):\n",
        "    return np.maximum(v, 0)\n",
        "\n",
        "  def vRelu_deriv(self, v):\n",
        "    return (v > 0) + 0.0\n",
        "\n",
        "  def vSoftmax(self, v):\n",
        "    s = np.exp(v)/sum(np.exp(v))\n",
        "    return s\n",
        "\n",
        "  def forward_prop(self, X):\n",
        "    \n",
        "    a = []\n",
        "    h = []\n",
        "\n",
        "    a.append( self.w[0].dot(X) + self.b[0] )\n",
        "    if self.act == \"sigmoid\":\n",
        "      h.append( self.vSigmoid(a[0]) )\n",
        "    elif self.act == \"tanh\":\n",
        "      h.append( self.vTanh(a[0]) )\n",
        "    elif self.act == \"relu\":\n",
        "      h.append( self.vRelu(a[0]) )\n",
        "    else:\n",
        "      print(\"fp else\")\n",
        "\n",
        "    for i in range(1,self.h_layers):\n",
        "      a.append( self.w[i].dot(h[i-1]) + self.b[i] )\n",
        "      if self.act == \"sigmoid\":\n",
        "        h.append( self.vSigmoid(a[i]) )\n",
        "      elif self.act == \"tanh\":\n",
        "        h.append( self.vTanh(a[i]) )\n",
        "      elif self.act == \"relu\":\n",
        "        h.append( self.vRelu(a[i]) )\n",
        "\n",
        "    a.append( self.w[self.h_layers].dot(h[self.h_layers-1]) + self.b[self.h_layers] )\n",
        "    h.append( self.vSoftmax(a[self.h_layers]) )\n",
        "\n",
        "    if self.opt == \"nesterov\":\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "        self.w[i] = self.w[i] - self.gamma * self.updateW[i]\n",
        "      for i in range(self.h_layers+1):\n",
        "        self.b[i] = self.b[i] - self.gamma * self.updateB[i]\n",
        "\n",
        "    if self.opt == \"nadam\":\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "        self.w[i] = self.w[i] - self.beta1 * self.mW[i]\n",
        "      for i in range(self.h_layers+1):\n",
        "        self.b[i] = self.b[i] - self.beta1 * self.mB[i]\n",
        "    return a, h\n",
        "\n",
        "  ## IMP -> one_hot_vector_size = no. of classes in classification problem, so change accordingly\n",
        "  def one_hot(self, Y, one_hot_vector_size = 10):\n",
        "\n",
        "    one_hot_y = np.zeros((Y.size, one_hot_vector_size))\n",
        "    one_hot_y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_y = one_hot_y.T\n",
        "    return one_hot_y\n",
        "\n",
        "  def back_prop(self, a, h, X, Y):\n",
        "\n",
        "    dA = []\n",
        "    dW = []\n",
        "    dB = []\n",
        "\n",
        "    one_hot_y = self.one_hot(Y)\n",
        "    dA.append( h[self.h_layers] - one_hot_y )\n",
        "    dW.append( 1/(X.shape[1]) * dA[0].dot(h[self.h_layers-1].T) + self.w_decay * self.w[self.h_layers] )\n",
        "    dB.append( 1/(X.shape[1]) * sum( dA[0].T).T.reshape(dA[0].shape[0],1) + self.w_decay * self.b[self.h_layers] )\n",
        "\n",
        "    for i in range(1,self.h_layers):\n",
        "\n",
        "      if self.act == \"sigmoid\":\n",
        "        dA.append( self.w[self.h_layers - i + 1].T.dot( dA[i-1] ) * self.vSigmoid_deriv(a[self.h_layers-i]))\n",
        "      elif self.act == \"tanh\":\n",
        "        dA.append( self.w[self.h_layers - i + 1].T.dot( dA[i-1] ) * self.vTanh_deriv(a[self.h_layers-i]))\n",
        "      elif self.act == \"relu\":\n",
        "        dA.append( self.w[self.h_layers - i + 1].T.dot( dA[i-1] ) * self.vRelu_deriv(a[self.h_layers-i]))\n",
        "\n",
        "      \n",
        "      dW.append( 1/(X.shape[1]) * dA[i].dot(h[self.h_layers-1-i].T) + self.w_decay * self.w[self.h_layers - i] )\n",
        "      dB.append( 1/(X.shape[1]) * sum( dA[i].T).T.reshape(dA[i].shape[0],1) + self.w_decay * self.b[self.h_layers - i] )\n",
        "\n",
        "    if self.act == \"sigmoid\":\n",
        "      dA.append( self.w[1].T.dot( dA[self.h_layers - 1] ) * self.vSigmoid_deriv(a[0]))\n",
        "    elif self.act == \"tanh\":\n",
        "      dA.append( self.w[1].T.dot( dA[self.h_layers - 1] ) * self.vTanh_deriv(a[0]))\n",
        "    elif self.act == \"relu\":\n",
        "      dA.append( self.w[1].T.dot( dA[self.h_layers - 1] ) * self.vRelu_deriv(a[0]))\n",
        "\n",
        "    dW.append( 1/(X.shape[1]) * dA[self.h_layers].dot(X.T) + self.w_decay * self.w[0] )\n",
        "    dB.append( 1/(X.shape[1]) * sum( dA[self.h_layers].T).T.reshape(dA[self.h_layers].shape[0],1) + self.w_decay * self.b[0] )\n",
        "\n",
        "    dw = []\n",
        "    for i in range(self.h_layers, -1, -1):\n",
        "      dw.append(dW[i])\n",
        "\n",
        "    db = []\n",
        "    for i in range(self.h_layers, -1, -1):\n",
        "      db.append(dB[i])\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "  def update_para(self, dW, dB):\n",
        "\n",
        "    if self.opt == \"adam\" or self.opt == \"nadam\":\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.mW[i] = self.beta1 * self.mW[i] + (1-self.beta1) * dW[i]\n",
        "        self.vW[i] = self.beta2 * self.vW[i] + (1-self.beta2) * dW[i]**2\n",
        "        self.w[i] = self.w[i] - self.eta/np.sqrt(self.vW[i] + self.eps) * self.mW[i]\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.mB[i] = self.beta1 * self.mB[i] + (1-self.beta1) * dB[i]\n",
        "        self.vB[i] = self.beta2 * self.vB[i] + (1-self.beta2) * dB[i]**2\n",
        "        self.b[i] = self.b[i] - self.eta/np.sqrt(self.vB[i] + self.eps) * self.mB[i]\n",
        "\n",
        "    elif self.opt == \"rmsprop\":\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.vW[i] = self.beta2 * self.vW[i] + (1-self.beta2) * dW[i]**2\n",
        "        self.w[i] = self.w[i] - self.eta/np.sqrt(self.vW[i] + self.eps) * dW[i]\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.vB[i] = self.beta2 * self.vB[i] + (1-self.beta2) * dB[i]**2\n",
        "        self.b[i] = self.b[i] - self.eta/np.sqrt(self.vB[i] + self.eps) * dB[i]\n",
        "\n",
        "    elif self.opt == \"nesterov\" or self.opt == \"momentum\":\n",
        "      \n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.updateW[i] = self.gamma * self.updateW[i] + self.eta * dW[i]\n",
        "        self.w[i] = self.w[i] - self.updateW[i]\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.updateB[i] = self.gamma * self.updateB[i] + self.eta * dB[i]\n",
        "        self.b[i] = self.b[i] - self.updateB[i]\n",
        "\n",
        "    else:\n",
        "      \n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.w[i] = self.w[i] - self.eta * dW[i]\n",
        "\n",
        "      for i in range(self.h_layers+1):\n",
        "\n",
        "        self.b[i] = self.b[i] - self.eta * dB[i]\n",
        "\n",
        "  def grad_descent(self, X, Y):\n",
        "\n",
        "    print(\"\\nOptimizer : \",self.opt)\n",
        "    print(\"Batch size : \",self.batch_size)\n",
        "    for i in range(self.epochs):\n",
        "\n",
        "      n = math.ceil(X.shape[1]/self.batch_size)-1\n",
        "\n",
        "      for j in range(n):\n",
        "\n",
        "        a, h = self.forward_prop(X.T[ self.batch_size * j : self.batch_size * (j+1)][:].T)\n",
        "        dW, dB = self.back_prop(a, h, X.T[ self.batch_size * j : self.batch_size * (j+1)][:].T, Y[ self.batch_size * j : self.batch_size * (j+1)])\n",
        "        self.update_para(dW, dB)\n",
        "\n",
        "      a, h = self.forward_prop(X.T[ self.batch_size * n : X.shape[1] ][:].T)\n",
        "      dW, dB = self.back_prop(a, h, X.T[ self.batch_size * n : X.shape[1] ][:].T, Y[ self.batch_size * n : X.shape[1] ])\n",
        "      self.update_para(dW, dB)\n",
        "\n",
        "  def accuracy(self, Y, y_hat):\n",
        "\n",
        "    count = 0\n",
        "    for i in range(y_hat.shape[0]):\n",
        "\n",
        "      if Y[i] == np.argmax(y_hat[i]):\n",
        "        count += 1\n",
        "    \n",
        "    accuracy = count/y_hat.shape[0]\n",
        "    print(f'Accuracy : {accuracy}')\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "424e2df7"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "smM5XmCycvLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fcde6e-f707-44bb-b32f-bb7c262cae0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xtrain shape  (60000, 784)\n",
            "xtest  shape  (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# reshaped x_train\n",
        "x_tr = x_train.reshape((x_train.shape[0], 784))/255\n",
        "print(\"xtrain shape \", x_tr.shape)\n",
        "\n",
        "# reshaped x_test\n",
        "x_ts = x_test.reshape((x_test.shape[0], 784))/255\n",
        "print(\"xtest  shape \", x_ts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 54000  # 90% of train data\n",
        "\n",
        "# for training\n",
        "x_tr1 = x_tr[0:m][:]\n",
        "y_tr1 = y_train[0:m]"
      ],
      "metadata": {
        "id": "T1QiE5cQYVln"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWu5FgAkHKio",
        "outputId": "2a71b16c-d4b6-42b8-8e47-98e3250b24cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimizer :  momentum\n",
            "Batch size :  145\n",
            "Validation Accuracy : 0.8516666666666667\n",
            "Train Accuracy : 0.8669814814814815\n",
            "Test Accuracy : 0.8501\n"
          ]
        }
      ],
      "source": [
        "# Skip this section if you want to perform wand sweeps.\n",
        "# This section is only for testing 1 configuration of parameters (no wandb intergration)\n",
        "\n",
        "shl = 32\n",
        "hl = 3\n",
        "hl_nodes = hl*[shl]\n",
        "\n",
        "#n1 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\", \"sgd\", 45, 0.8, batch_size = 145)\n",
        "n1 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\", \"momentum\", 10, 0.9,batch_size = 145)\n",
        "#n3 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\", \"nesterov\", 40, 0.8, 145)\n",
        "#n4 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\", \"rmsprop\", 45, 0.0005,batch_size = 145)\n",
        "#n5 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\",  \"adam\", 45, 0.0005,batch_size = 145)\n",
        "#n6 = network(x_tr1.shape[1], 10, hl_nodes, \"sigmoid\", \"nadam\", 40, 0.0005,batch_size = 145)\n",
        "\n",
        "n1.grad_descent(x_tr1.T, y_tr1)\n",
        "\n",
        "# validation data = 10% of train data = 6000\n",
        "# accuracy measure\n",
        "a, h = n1.forward_prop(x_tr[54000:60000].T)\n",
        "print(\"Validation \",end='')\n",
        "accuracy = n1.accuracy(y_train[54000:60000], h[len(hl_nodes)].T)\n",
        "          \n",
        "a, h = n1.forward_prop(x_tr1[0:m].T)\n",
        "print(\"Train \",end='')\n",
        "_ = n1.accuracy(y_tr1[0:m], h[len(hl_nodes)].T)\n",
        "\n",
        "a, h = n1.forward_prop(x_ts.T)\n",
        "print(\"Test \",end='')\n",
        "_ = n1.accuracy(y_test, h[len(hl_nodes)].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26z66ajS8u39"
      },
      "outputs": [],
      "source": [
        "# Code from here is for logging wandb sweeps.\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JsM8B2O-SXP"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iust15TjwLlx"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'valid_acc',\n",
        "    'goal': 'maximize'   \n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'epochs': {\n",
        "        'values': [5, 10]\n",
        "        },\n",
        "    'no_of_HL': {\n",
        "        'values': [3, 4, 5]\n",
        "        },\n",
        "    'HL_size': {\n",
        "        'values': [32, 64, 128]\n",
        "        },\n",
        "    'w_decay': {\n",
        "        'values': [0, 0.5, 0.05, 0.005, 0.0005]\n",
        "        },\n",
        "    'eta': {\n",
        "        'values': [1e-3, 1e-4]  \n",
        "        },\n",
        "    'optimizer': {\n",
        "        'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'values': [16, 32, 64, 128]  \n",
        "        },\n",
        "    'activation': {\n",
        "        'values': ['sigmoid', 'tanh', 'relu']\n",
        "        }\n",
        "}\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DL-Assignment-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SCcMOJ5CDHDD"
      },
      "outputs": [],
      "source": [
        "# train function for wandb\n",
        "def train(config = None):\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  with wandb.init(config=config):\n",
        "    # If called by wandb.agent, as below,\n",
        "    # this config will be set by Sweep Controller\n",
        "    config = wandb.config\n",
        "\n",
        "    # list of no. of nuerons in each hidden layer\n",
        "    hl_nodes = config.no_of_HL*[config.HL_size]\n",
        "\n",
        "    #(il, ol, hls, activation, optimizer, epochs, eta, batch_size, w_decay = 0, gamma = 0.9, beta1=0.9, beta2 = 0.999, eps = 1e-8)\n",
        "    n1 = network(x_tr1.shape[1], 10, hl_nodes, config.activation, config.optimizer, config.epochs, config.eta, config.batch_size, config.w_decay)\n",
        "\n",
        "    a, h = n1.forward_prop(x_tr[54000:60000].T)\n",
        "    accuracy = n1.accuracy(y_train[54000:60000], h[len(hl_nodes)].T)\n",
        "    wandb.log({\"valid_acc\": accuracy, \"step\": 0})\n",
        "    wandb.log({\"valid_loss\": 1-accuracy, \"step\": 0})\n",
        "              \n",
        "    a, h = n1.forward_prop(x_ts.T)\n",
        "    acc = n1.accuracy(y_test, h[len(hl_nodes)].T)\n",
        "    wandb.log({\"test_acc\": acc, \"step\": 0})\n",
        "    wandb.log({\"test_loss\": 1-acc, \"step\": 0})\n",
        "    wandb.log({\"epochs\": 0, \"step\": 0})\n",
        "\n",
        "    n1.grad_descent(x_tr1.T, y_tr1)\n",
        "\n",
        "    a, h = n1.forward_prop(x_tr[54000:60000].T)\n",
        "    print(\"Validation \",end='')\n",
        "    accuracy = n1.accuracy(y_train[54000:60000], h[len(hl_nodes)].T)\n",
        "    wandb.log({\"valid_acc\": accuracy, \"step\": 1})\n",
        "    wandb.log({\"valid_loss\": 1-accuracy, \"step\": 1})\n",
        "              \n",
        "    a, h = n1.forward_prop(x_tr1[0:m].T)\n",
        "    print(\"Train \",end='')\n",
        "    _ = n1.accuracy(y_tr1[0:m], h[len(hl_nodes)].T)\n",
        "\n",
        "    a, h = n1.forward_prop(x_ts.T)\n",
        "    print(\"Test \",end='')\n",
        "    acc = n1.accuracy(y_test, h[len(hl_nodes)].T)\n",
        "    wandb.log({\"test_acc\": acc, \"step\": 1})\n",
        "    wandb.log({\"test_loss\": 1-acc, \"step\": 1})\n",
        "    wandb.log({\"epochs\": config.epochs, \"step\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkI0HmyzeskW"
      },
      "outputs": [],
      "source": [
        "#executing wandb agent\n",
        "wandb.agent(sweep_id, train, count= 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A1_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}